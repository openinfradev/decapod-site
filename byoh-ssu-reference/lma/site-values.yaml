apiVersion: openinfradev.github.com/v1
kind: HelmValuesTransformer
metadata:
  name: site

# 대상환경의 정보를 넣는 부분 (btv경우)
# - clusterName 이건 tks-api가 해줄꺼 같음
# - s3Service 이것도 tks-api가 해줄꺼고 ep secret을 잘 넣어줘야함
# - lmaNameSpace 이건 그냥 하던대로 놓는걸로..
# - grafanaDatasourceMetric 이것도 ep secret
# ......
# - kafka 정보가 들어오면 이것도 추가해 수정필요, 예시 형태로 추가해둠, 'name: fluentbit' 검색된 블록에 주석부분 확인
# - kafka로 보내기는 routing 없이 모두다 kafka에 싣습니다.
# ============================================================================
# 환경에 맞추어 다음과 같이 수정해둠
# minio에 2000GB 할당, 이전에 es에 3개의 데이터 노드에 그렇게 돼있던걸로 생각해 설정했으나 그렇지 않다면 변경필요 -> 'name: minio' 검색된 블록에서 persistance 부분
# 위 내역을 수정하면 적절하게 subcommand 부분도 수정해야함
# 현제 설정은 매트릭 7일, 로그 2일 보관

global:
  nodeSelector:
    taco-lma: enabled
  clusterName: cluster.local
  storageClassName: taco-storage
  repository: https://openinfradev.github.io/helm-repo/
  serviceScrapeInterval: 30s
  defaultPassword: password
  defaultUser: taco
  thanosObjstoreSecret: taco-objstore-secret

  lokiHost: loki-loki-distributed-gateway
  lokiPort: 80
  s3Service: "minio.lma.svc:9000" # depends on $lmaNameSpace (ex. minio.taco-system.svc)

  lmaNameSpace: lma

  TksWebhookUrl: "FixItByWF"
  SlackUrl: "FixItByWF"
  SlackChannel: '#temporary-alert'

  grafanaDatasourceMetric: lma-prometheus:9090
  thanosQueryStores:
  - thanos-storegateway:10901
  - prometheus-operated:10901

  # servicemesh dashboard and grafana
  realms: 04a70f29
  consoleUrl: tks-console.taco-cat.xyz
  grafanaDomain: taco-cat.xyz
  keycloakDomain: keycloak-eom.taco-cat.xyz
  grafanaClientSecret: JLtsanYtrCg21RGxrcVmQP0GeuDFUhpA

  awsNlbAnnotation:
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
    service.beta.kubernetes.io/aws-load-balancer-type: nlb

  tksIamRoles: []

charts:
- name: prometheus-operator
  override:
    prometheusOperator.nodeSelector: $(nodeSelector)
    prometheusOperator.admissionWebhooks.patch.image.sha: ""
    prometheusOperator.image.repository: tks/prometheus-operator
    prometheusOperator.admissionWebhooks.patch.image.repository: tks/kube-webhook-certgen
    prometheusOperator.prometheusConfigReloader.image.repository: tks/prometheus-config-reloader
    prometheusOperator.thanosImage.repository: tks/thanos

- name: prometheus
  override:
    kubeEtcd.enabled: true
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName: $(storageClassName)
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage: 20Gi
    prometheus.prometheusSpec.retention: 2d
    prometheus.prometheusSpec.externalLabels.taco_cluster: $(clusterName)
    prometheus.prometheusSpec.nodeSelector: $(nodeSelector)
    prometheus.prometheusSpec.serviceMonitorNamespaceSelector.matchLabels.name: $(lmaNameSpace)
    prometheus.prometheusSpec.podMonitorNamespaceSelector.matchLabels.name: $(lmaNameSpace)
    prometheus.prometheusSpec.ruleNamespaceSelector.matchLabels.name: $(lmaNameSpace)
    prometheus.thanosServiceExternal.annotations: $(awsNlbAnnotation)
    prometheus.thanosServiceExternal:
      type: NodePort
      nodePort: 30004
    alertmanager.service.type: NodePort
    alertmanager.service.nodePort: 30111
    alertmanager.alertmanagerSpec.alertmanagerConfigSelector.matchLabels.alertmanagerConfig: example
    alertmanager.alertmanagerSpec.nodeSelector: $(nodeSelector)
    alertmanager.alertmanagerSpec.retention: 2h
    alertmanager.config:
      global:
        slack_api_url: $(SlackUrl)
      receivers:
      - name: tks-alert
        webhook_configs:
        - send_resolved: true
          url: $(TksWebhookUrl)
      route:
        group_by:
        - alertname
        group_wait: 10s
        receiver: tks-alert
        repeat_interval: 1h

- name: prometheus-node-exporter


- name: kube-state-metrics
  override:
    nodeSelector: $(nodeSelector)
    kubeVersion: v1.25.7

- name: prometheus-pushgateway
  override:
    nodeSelector: $(nodeSelector)

- name: prometheus-process-exporter
  override:
    conf.processes: dockerd,kubelet,kube-proxy,ntpd,node

- name: grafana
  override:
    adminPassword: password
    persistence.storageClassName: $(storageClassName)
    sidecar.dashboards.searchNamespace: ALL
    # grafana oidc
    service:
      type: NodePort
      nodePort: 30001
    service.annotations: $(awsNlbAnnotation)
    grafana\.ini:
      server:
        domain: $(grafanaDomain)
        root_url: http://$(grafanaDomain)
        serve_from_sub_path: true
      auth.generic_oauth:
        enabled: true
        name: keycloak
        allow_sign_up: true
        client_id: grafana
        client_secret: $(grafanaClientSecret)
        scopes: openid profile email
        login_attribute_path: username
        auth_url: https://$(keycloakDomain)/auth/realms/$(realms)/protocol/openid-connect/auth
        token_url: https://$(keycloakDomain)/auth/realms/$(realms)/protocol/openid-connect/token
        api_url: https://$(keycloakDomain)/auth/realms/$(realms)/protocol/openid-connect/userinfo
        signout_redirect_url: $(consoleUrl)/login
      auth:
        disable_login_form: false
        oauth_auto_login: true
        disable_signout_menu: true
      user:
        auto_assign_org: true
        auto_assign_org_role: Admin

- name: fluent-operator

- name: fluentbit
  override:
    fluentbit:
      clusterName: $(clusterName)
      outputs:
        loki:
        - name: taco-loki
          host: $(lokiHost)
          port: $(lokiPort)
        # kafka:
        #   enabled: true
        #   broker: btv-kafka-0.my-kafka-headless.lma.svc.cluster.local:9092,
        #   topics: taco

      targetLogs:
      - tag: kube.*
        bufferChunkSize: 2M
        bufferMaxSize: 5M
        do_not_store_as_default: true
        index: container
        loki_name: taco-loki
        memBufLimit: 20MB
        multi_index:
        - index: platform
          loki_name: taco-loki
          key: $kubernetes['namespace_name']
          value: kube-system|$(lmaNameSpace)|taco-system|gatekeeper-system|argo
        parser: docker
        path: /var/log/containers/*.log
        type: kubernates
        extraArgs:
          multilineParser: docker, cri
      - tag: syslog.*
        loki_name: taco-loki
        index: syslog
        parser: taco-syslog-parser-for-ubuntu
        path: /var/log/messages
        type: syslog

- name: addons
  override:
    SPECIAL_VALUE: SPECIAL
    serviceMonitor.trident:
      enabled: false
      interval: $(serviceScrapeInterval)
    serviceMonitor.kubelet.interval: 30s
    serviceMonitor.additionalScrapeConfigs:
    grafanaDashboard.istio.enabled: false
    grafanaDashboard.jaeger.enabled: false
    grafanaDashboard.namespace: $(lmaNameSpace)
    grafanaDatasource.namespace: $(lmaNameSpace)
    serviceMonitor.istio.enabled: false
    serviceMonitor.jaeger.enabled: false
    serviceMonitor.argocd.enabled: false
    serviceMonitor.argowf.enabled: false
    prometheusRules.alert.enabled: false
    prometheusRules.istio.aggregation.enabled: false
    prometheusRules.istio.optimization.enabled: false
    grafanaDatasource.prometheus.url: $(grafanaDatasourceMetric)
    # grafanaDatasource.prometheus.url: "thanos-query.lma:9090"
    grafanaDatasource.loki.url: $(lokiHost):$(lokiPort)

- name: prometheus-adapter
  override:
    nodeSelector: $(nodeSelector)

- name: kubernetes-event-exporter
  override:
    clustername: $(clusterName)

    conf.recievers:
      - name: loki
        type: file
        config:
          path: "/tmp/kubernetes-event.log"
    addons:
      loki:
        enabled: true
        host: $(lokiHost)
        port: $(lokiPort)
        target_file: "/tmp/kubernetes-event.log"
    conf.default.hosts:
    - "https://eck-elasticsearch-es-http.lma.svc.$(clusterName):9200"

- name: minio
  override:
    users:
      - accessKey: $(defaultUser)
        secretKey: $(defaultPassword)
        policy: consoleAdmin
    buckets:
      - name: tks-thanos
        policy: public
        purge: false
        versioning: true
        objectlocking: false
      - name: tks-loki
        policy: public
        purge: false
        versioning: true
        objectlocking: false
    persistence.storageClass: $(storageClassName)
    persistence.size: 2000Gi
    persistence.accessMode: ReadWriteOnce
    service:
      type: NodePort
      nodePort: 30003
    service.annotations: $(awsNlbAnnotation)
    # deploy target node's label
    consoleIngress.nodeSelector: $(nodeSelector)
    postJob.nodeSelector: $(nodeSelector)
    customCommands:
    - command: ilm rule add --expire-days 7 myminio/tks-thanos
    - command: ilm rule add --expire-days 2 myminio/tks-loki
    - command: ilm ls myminio/tks-thanos
    - command: ilm ls myminio/tks-loki

- name: thanos
  override:
    global.storageClass: $(storageClassName)
    # temporarily add annotation because a cluster is using not cluster-name but 'cluster.local'
    # clusterDomain: $(clusterName)
    existingObjstoreSecret: $(thanosObjstoreSecret)
    query.nodeSelector: $(nodeSelector)
    query.service.type: ClusterIP
    query.service.annotations: $(awsNlbAnnotation)
    queryFrontend.nodeSelector: $(nodeSelector)
    queryFrontend.service:
      type: NodePort
      nodePort: 30005
    queryFrontend.enabled: true
    queryFrontend.config: |-
        type: IN-MEMORY
        config:
          max_size: 512MB
          max_size_items: 100
          validity: 100s
    queryFrontend.extraFlags: []
    querier.stores: $(thanosQueryStores)
    bucketweb.nodeSelector: $(nodeSelector)
    compactor.nodeSelector: $(nodeSelector)
    storegateway.nodeSelector: $(nodeSelector)
    compactor.persistence.size: 8Gi
    # compactor.extraFlags:
    # - --compact.enable-vertical-compaction
    # - --deduplication.replica-label="replica"
    storegateway.persistence.size: 8Gi
    ruler.nodeSelector: $(nodeSelector)
    ruler.alertmanagers:
    - http://alertmanager-operated:9093
    ruler.persistence.size: 8Gi
    ruler.config:
      groups:
        - name: "tks"
          rules:
          - alert: "PrometheusDown"
            expr: absent(up{prometheus="lma/lma-prometheus"})
          - alert: node-cpu-high-load
            annotations:
              message: 클러스터({{ $labels.taco_cluster }})의 노드({{ $labels.instance }})의 idle process의 cpu 점유율이 3분 동안 0% 입니다. (현재 사용률 {{$value}})
              description: 워커 노드 CPU가 과부하 상태입니다. 일시적인 서비스 Traffic 증가, Workload의 SW 오류, Server HW Fan Fail등 다양한 원인으로 인해 발생할 수 있습니다.
              Checkpoint: 일시적인 Service Traffic의 증가가 관측되지 않았다면, Alert발생 노드에서 실행 되는 pod중 CPU 자원을 많이 점유하는 pod의 설정을 점검해 보시길 제안드립니다. 예를 들어 pod spec의 limit 설정으로 과도한 CPU자원 점유을 막을 수 있습니다.
              summary: Cpu resources of the node {{ $labels.instance }} are running low.
              discriminative: $labels.taco_cluster, $labels.instance
            expr: (avg by (taco_cluster, instance) (rate(node_cpu_seconds_total{mode="idle"}[60s]))) < 0 #0.1 # 진짜 0?
            for: 3m
            labels:
              severity: warning
          - alert: node-memory-high-utilization
            annotations:
              message: 클러스터({{ $labels.taco_cluster }})의 노드({{ $labels.instance }})의 Memory 사용량이 3분동안 80% 를 넘어서고 있습니다. (현재 사용률 {{$value}})
              descriptioon: 워커 노드의 Memory 사용량이 80%를 넘었습니다. 일시적인 서비스 증가 및 SW 오류등 다양한 원인으로 발생할 수 있습니다.
              Checkpoint: 일시적인 Service Traffic의 증가가 관측되지 않았다면, Alert발생 노드에서 실행되는 pod중 Memory 사용량이 높은 pod들에 대한 점검을 제안드립니다.
              summary: Memory resources of the node {{ $labels.instance }} are running low.
              discriminative: $labels.taco_cluster, $labels.instance
            expr: (node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) <  0.2
            for: 3m
            labels:
              severity: warning
          - alert: node-disk-full
            annotations:
              message: 지난 6시간동안의 추세로 봤을 때, 클러스터({{ $labels.taco_cluster }})의 노드({{ $labels.instance }})의 root 볼륨은 24시간 안에 Disk full이 예상됨
              description: 현재 Disk 사용 추세기준 24시간 내에 Disk 용량이 꽉 찰 것으로 예상됩니다.
              Checkpoint: Disk 용량 최적화(삭제 및 Backup)을 수행하시길 권고합니다. 삭제할 내역이 없으면 증설 계획을 수립해 주십시요.
              summary: Memory resources of the node {{ $labels.instance }} are running low.
              discriminative: $labels.taco_cluster, $labels.instance
            expr: predict_linear(node_filesystem_free_bytes{mountpoint="/"}[6h], 24*3600) < 0
            for: 30m
            labels:
              severity: critical
          - alert: pvc-full
            annotations:
              message: 지난 6시간동안의 추세로 봤을 때, 클러스터({{ $labels.taco_cluster }})의 파드({{ $labels.persistentvolumeclaim }})가 24시간 안에 Disk full이 예상됨
              description: 현재 Disk 사용 추세기준 24시간 내에 Disk 용량이 꽉 찰것으로 예상됩니다. ({{ $labels.taco_cluster }} 클러스터, {{ $labels.persistentvolumeclaim }} PVC)
              Checkpoint: Disk 용량 최적화(삭제 및 Backup)을 수행하시길 권고합니다. 삭제할 내역이 없으면 증설 계획을 수립해 주십시요.
              summary: Disk resources of the volume(pvc) {{ $labels.persistentvolumeclaim }} are running low.
              discriminative: $labels.taco_cluster, $labels.persistentvolumeclaim
            expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 24*3600) < 0 # kubelet_volume_stats_capacity_bytes
            for: 30m
            labels:
              severity: critical
          - alert: pod-restart-frequently
            annotations:
              message: 클러스터({{ $labels.taco_cluster }})의 파드({{ $labels.pod }})가 30분 동안 5회 이상 재기동 ({{ $value }}회)
              description: 특정 Pod가 빈번하게 재기동 되고 있습니다. 점검이 필요합니다. ({{ $labels.taco_cluster }} 클러스터, {{ $labels.pod }} 파드)
              Checkpoint: pod spec. 에 대한 점검이 필요합니다. pod의 log 및 status를 확인해 주세요.
              discriminative: $labels.taco_cluster, $labels.pod, $labels.namespace
            expr: increase(kube_pod_container_status_restarts_total{namespace!="kube-system"}[60m:]) > 2 # 몇회로 할 것인지?
            for: 30m
            labels:
              severity: critical

- name: thanos-config
  override:
    objectStorage:
      secretName: $(thanosObjstoreSecret)
      rawConfig:
        bucket: tks-thanos
        endpoint: $(s3Service)
        access_key: $(defaultUser)
        secret_key: $(defaultPassword)
        insecure: true
    sidecarsService.name: thanos-sidecars
    sidecarsService.endpoints:
      - 192.168.97.102 # should not be in the loopback range (127.0.0.0/8)

- name: prepare-etcd-secret
  override:
    nodeSelector:
      "node-role.kubernetes.io/control-plane": ""
    tolerations:
      - key: "node-role.kubernetes.io/control-plane"
        effect: "NoSchedule"
        operator: "Exists"

- name: loki
  override:
    global.dnsService: kube-dns
    # global.clusterDomain: $(clusterName) # annotate cluste because the cluster name is still cluster.local regardless cluster
    gateway.service:
      type: NodePort
      nodePort: 30002
    gateway.service.annotations: $(awsNlbAnnotation)
    ingester.persistence.storageClass: $(storageClassName)
    distributor.persistence.storageClass: $(storageClassName)
    queryFrontend.persistence.storageClass: $(storageClassName)
    ruler.persistence.storageClass: $(storageClassName)
    indexGateway.persistence.storageClass: $(storageClassName)
    # select target node's label
    ingester.nodeSelector: $(nodeSelector)
    distributor.nodeSelector: $(nodeSelector)
    querier.nodeSelector: $(nodeSelector)
    queryFrontend.nodeSelector: $(nodeSelector)
    queryScheduler.nodeSelector: $(nodeSelector)
    tableManager.nodeSelector: $(nodeSelector)
    gateway.nodeSelector: $(nodeSelector)
    compactor.nodeSelector: $(nodeSelector)
    ruler.nodeSelector: $(nodeSelector)
    indexGateway.nodeSelector: $(nodeSelector)
    memcachedChunks.nodeSelector: $(nodeSelector)
    memcachedFrontend.nodeSelector: $(nodeSelector)
    memcachedIndexQueries.nodeSelector: $(nodeSelector)
    memcachedIndexWrites.nodeSelector: $(nodeSelector)
    loki:
      storageConfig:
        aws:
          s3: http://$(defaultUser):$(defaultPassword)@$(s3Service)/minio

- name: lma-bucket
  override:
    s3.enabled: true
    s3.buckets:
    - name: $(clusterName)-tks-thanos
    - name: $(clusterName)-tks-loki
    tks.iamRoles: $(tksIamRoles)
