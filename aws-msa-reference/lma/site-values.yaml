apiVersion: openinfradev.github.com/v1
kind: HelmValuesTransformer
metadata:
  name: site

global:
  nodeSelector:
    taco-lma: enabled
  clusterName: siim-dev
  storageClassName: taco-storage
  repository: https://openinfradev.github.io/helm-repo/
  serviceScrapeInterval: 30s
  defaultPassword: tacoword
  defaultUser: taco
  thanosObjstoreSecret: taco-objstore-secret

  lokiHost: loki-loki-distributed-gateway
  lokiPort: 80
  s3Service: "minio.taco-system.svc:9000" # depends on $lmaNameSpace (ex. minio.taco-system.svc)

  lmaNameSpace: taco-system

  TksWebhookUrl: "FixItByWF"
  SlackUrl: https://hooks.slack.com/services/T0WU4JZEX/BGK2W2NUF/ #y2S2Y8FjfkbBs1eQ6u2Y6b8L #"FixItByWF"
  SlackChannel: '#temporary-alert'

  grafanaDatasourceMetric: thanos-query-frontend:9090
  thanosQueryStores:
  - thanos-storegateway:10901
  - prometheus-operated:10901

  # servicemesh dashboard and grafana
  realms: 04a70f29
  consoleUrl: tks-console.taco-cat.xyz
  grafanaDomain: taco-cat.xyz
  keycloakDomain: keycloak-eom.taco-cat.xyz
  grafanaClientSecret: JLtsanYtrCg21RGxrcVmQP0GeuDFUhpA

  awsNlbAnnotation:
    service.beta.kubernetes.io/aws-load-balancer-proxy-protocol: '*'
    service.beta.kubernetes.io/aws-load-balancer-type: nlb

  tksIamRoles: []

charts:
- name: prometheus-operator
  override:
    prometheusOperator.nodeSelector: $(nodeSelector)
    prometheusOperator.admissionWebhooks.patch.image.sha: ""
    prometheusOperator.image.repository: tks/prometheus-operator
    prometheusOperator.admissionWebhooks.patch.image.repository: tks/kube-webhook-certgen
    prometheusOperator.prometheusConfigReloader.image.repository: tks/prometheus-config-reloader
    prometheusOperator.thanosImage.repository: tks/thanos

- name: prometheus
  override:
    kubeEtcd.enabled: true
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.storageClassName: $(storageClassName)
    prometheus.prometheusSpec.storageSpec.volumeClaimTemplate.spec.resources.requests.storage: 20Gi
    prometheus.prometheusSpec.retention: 2d
    prometheus.prometheusSpec.externalLabels.taco_cluster: $(clusterName)
    prometheus.prometheusSpec.nodeSelector: $(nodeSelector)
    prometheus.prometheusSpec.serviceMonitorNamespaceSelector.matchLabels.name: $(lmaNameSpace)
    prometheus.prometheusSpec.podMonitorNamespaceSelector.matchLabels.name: $(lmaNameSpace)
    prometheus.prometheusSpec.ruleNamespaceSelector.matchLabels.name: $(lmaNameSpace)
    prometheus.thanosServiceExternal.annotations: $(awsNlbAnnotation)
    prometheus.thanosServiceExternal.type: NodePort
    prometheus.thanosServiceExternal.NodePort: 30005

    alertmanager.service.type: NodePort
    alertmanager.service.nodePort: 30111
    alertmanager.alertmanagerSpec.alertmanagerConfigSelector.matchLabels.alertmanagerConfig: example
    alertmanager.alertmanagerSpec.nodeSelector: $(nodeSelector)
    alertmanager.alertmanagerSpec.retention: 2h
    alertmanager.config:
      global:
        slack_api_url: $(SlackUrl)
      receivers:
      - name: default-alert
        slack_configs:
        - send_resolved: true
          channel: '#alert'
          username: Prometheus
          title: '{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing
            | len }}{{ end }}] {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved)
            0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range
            .Alerts.Firing }}{{ .Labels.alertname }}{{ end }}{{ range .Alerts.Resolved
            }}{{ .Labels.alertname }}{{ end }}{{ end }}'
          text: |-
            {{ if or (and (eq (len .Alerts.Firing) 1) (eq (len .Alerts.Resolved) 0)) (and (eq (len .Alerts.Firing) 0) (eq (len .Alerts.Resolved) 1)) }} {{ range .Alerts.Firing }}{{ .Annotations.message }}{{ end }}{{ range .Alerts.Resolved }}{{ .Annotations.message  }}{{ end }}
            {{ else }}
            {{ if gt (len .Alerts.Firing) 0 }}
            *Alerts Firing:*
              {{ range .Alerts.Firing }}- {{ .Labels.alertname  }}: {{ .Annotations.message }}
            {{ end }}{{ end }}
            {{ if gt (len .Alerts.Resolved) 0 }}
            *Alerts Resolved:*
              {{ range .Alerts.Resolved }}- {{ .Labels.alertname }}: {{ .Annotations.message }}
            {{ end }}{{ end }}
            {{ end }}
      route:
        group_by:
        - alertname
        group_wait: 10s
        receiver: default-alert
        repeat_interval: 1h

- name: prometheus-node-exporter
  override:
    hostNetwork: false

- name: kube-state-metrics
  override:
    nodeSelector: $(nodeSelector)
    kubeVersion: v1.25.7

- name: prometheus-pushgateway
  override:
    nodeSelector: $(nodeSelector)

- name: prometheus-process-exporter
  override:
    conf.processes: dockerd,kubelet,kube-proxy,ntpd,node
    pod.hostNetwork: false

- name: grafana
  override:
    adminPassword: tacoword
    persistence.storageClassName: $(storageClassName)
    sidecar.dashboards.searchNamespace: ALL
    # grafana oidc
    service.type: NodePort

    service.annotations: $(awsNlbAnnotation)
    grafana\.ini:
      server:
        domain: $(grafanaDomain)
        root_url: http://$(grafanaDomain)
        serve_from_sub_path: true
      auth.generic_oauth:
        enabled: true
        name: keycloak
        allow_sign_up: true
        client_id: grafana
        client_secret: $(grafanaClientSecret)
        scopes: openid profile email
        login_attribute_path: username
        auth_url: https://$(keycloakDomain)/auth/realms/$(realms)/protocol/openid-connect/auth
        token_url: https://$(keycloakDomain)/auth/realms/$(realms)/protocol/openid-connect/token
        api_url: https://$(keycloakDomain)/auth/realms/$(realms)/protocol/openid-connect/userinfo
        signout_redirect_url: $(consoleUrl)/login
      # auth:
      #   disable_login_form: false
      #   oauth_auto_login: true
      #   disable_signout_menu: true
      user:
        auto_assign_org: true
        auto_assign_org_role: Admin

- name: fluent-operator

- name: fluentbit
  override:
    fluentbit:
      clusterName: $(clusterName)
      outputs:
        loki:
        - name: taco-loki
          host: $(lokiHost)
          port: $(lokiPort)
      targetLogs:
      - tag: kube.*
        bufferChunkSize: 2M
        bufferMaxSize: 5M
        do_not_store_as_default: false
        index: container
        loki_name: taco-loki
        memBufLimit: 20MB
        multi_index:
        - index: platform
          loki_name: taco-loki
          key: $kubernetes['namespace_name']
          value: kube-system|$(lmaNameSpace)|taco-system|argo
        parser: docker
        path: /var/log/containers/*.log
        type: kubernates
        extraArgs:
          multilineParser: docker, cri
      - tag: syslog.*
        loki_name: taco-loki
        index: syslog
        parser: taco-syslog-parser-for-ubuntu
        path: /var/log/syslog
        type: syslog

- name: addons
  override:
    SPECIAL_VALUE: SPECIAL
    serviceMonitor.trident:
      enabled: false
      interval: $(serviceScrapeInterval)
    serviceMonitor.kubelet.interval: 30s
    serviceMonitor.additionalScrapeConfigs:
    grafanaDashboard.istio.enabled: false
    grafanaDashboard.jaeger.enabled: false
    grafanaDashboard.namespace: $(lmaNameSpace)
    grafanaDatasource.namespace: $(lmaNameSpace)
    serviceMonitor.istio.enabled: false
    serviceMonitor.jaeger.enabled: false
    serviceMonitor.argocd.enabled: false
    serviceMonitor.argowf.enabled: false
    prometheusRules.alert.enabled: false
    prometheusRules.istio.aggregation.enabled: false
    prometheusRules.istio.optimization.enabled: false
    grafanaDatasource.prometheus.url: $(grafanaDatasourceMetric)
    # grafanaDatasource.prometheus.url: "thanos-query.lma:9090"
    grafanaDatasource.loki.url: $(lokiHost):$(lokiPort)

- name: prometheus-adapter
  override:
    nodeSelector: $(nodeSelector)

- name: kubernetes-event-exporter
  override:
    clustername: $(clusterName)

    conf.recievers:
      - name: loki
        type: file
        config:
          path: "/tmp/kubernetes-event.log"
    addons:
      loki:
        enabled: true
        host: $(lokiHost)
        port: $(lokiPort)
        target_file: "/tmp/kubernetes-event.log"
    conf.default.hosts:
    - "https://eck-elasticsearch-es-http.lma.svc.$(clusterName):9200"

- name: minio
  override:
    # rootUser: $(defaultUser)
    # rootPassword: $(defaultPassword)
    clusterDomain: $(clusterName)
    users:
      - accessKey: $(defaultUser)
        secretKey: $(defaultPassword)
        policy: consoleAdmin
    buckets:
      - name: tks-thanos
        policy: public
        purge: false
        versioning: true
        objectlocking: false
      - name: tks-loki
        policy: public
        purge: false
        versioning: true
        objectlocking: false
    persistence.storageClass: $(storageClassName)
    persistence.size: 500Gi
    persistence.accessMode: ReadWriteOnce
    service.type: ClusterIP
    service.annotations: $(awsNlbAnnotation)
    customCommands:
    - command: ilm rule add --expire-days 90 myminio/tks-thanos
    - command: ilm rule add --expire-days 15 myminio/tks-loki
    - command: ilm ls myminio/tks-thanos
    - command: ilm ls myminio/tks-loki
    # deploy target node's label
    consoleIngress.nodeSelector: $(nodeSelector)
    postJob.nodeSelector: $(nodeSelector)

- name: thanos
  override:
    global.storageClass: $(storageClassName)
    # temporarily add annotation because a cluster is using not cluster-name but 'cluster.local'
    clusterDomain: $(clusterName)
    existingObjstoreSecret: $(thanosObjstoreSecret)
    query.nodeSelector: $(nodeSelector)
    query.service.type: ClusterIP
    query.service.annotations: $(awsNlbAnnotation)
    queryFrontend.nodeSelector: $(nodeSelector)
    queryFrontend.service.type: ClusterIP
    queryFrontend.enabled: true
    queryFrontend.config: |-
        type: IN-MEMORY
        config:
          max_size: 512MB
          max_size_items: 100
          validity: 100s
    queryFrontend.extraFlags: []
    querier.stores: $(thanosQueryStores)
    bucketweb.nodeSelector: $(nodeSelector)
    compactor.nodeSelector: $(nodeSelector)
    storegateway.nodeSelector: $(nodeSelector)
    compactor.persistence.size: 8Gi
    # compactor.extraFlags:
    # - --compact.enable-vertical-compaction
    # - --deduplication.replica-label="replica"
    storegateway.persistence.size: 8Gi
    ruler.nodeSelector: $(nodeSelector)
    ruler.alertmanagers:
    - http://alertmanager-operated:9093
    ruler.persistence.size: 8Gi
    ruler.config:
      groups:
        - name: "tks"
          rules:
          - alert: "PrometheusDown"
            expr: absent(up{prometheus="lma/lma-prometheus"})
          - alert: node-cpu-high-load
            annotations:
              message: 클러스터({{ $labels.taco_cluster }})의 노드({{ $labels.instance }})의 idle process의 cpu 점유율이 3분 동안 0% 입니다. (현재 사용률 {{$value}})
              description: 워커 노드 CPU가 과부하 상태입니다. 일시적인 서비스 Traffic 증가, Workload의 SW 오류, Server HW Fan Fail등 다양한 원인으로 인해 발생할 수 있습니다.
              Checkpoint: 일시적인 Service Traffic의 증가가 관측되지 않았다면, Alert발생 노드에서 실행 되는 pod중 CPU 자원을 많이 점유하는 pod의 설정을 점검해 보시길 제안드립니다. 예를 들어 pod spec의 limit 설정으로 과도한 CPU자원 점유을 막을 수 있습니다.
              summary: Cpu resources of the node {{ $labels.instance }} are running low.
              discriminative: $labels.taco_cluster, $labels.instance
            expr: (avg by (taco_cluster, instance) (rate(node_cpu_seconds_total{mode="idle"}[60s]))) < 0 #0.1 # 진짜 0?
            for: 3m
            labels:
              severity: warning
          - alert: node-memory-high-utilization
            annotations:
              message: 클러스터({{ $labels.taco_cluster }})의 노드({{ $labels.instance }})의 Memory 사용량이 3분동안 80% 를 넘어서고 있습니다. (현재 사용률 {{$value}})
              descriptioon: 워커 노드의 Memory 사용량이 80%를 넘었습니다. 일시적인 서비스 증가 및 SW 오류등 다양한 원인으로 발생할 수 있습니다.
              Checkpoint: 일시적인 Service Traffic의 증가가 관측되지 않았다면, Alert발생 노드에서 실행되는 pod중 Memory 사용량이 높은 pod들에 대한 점검을 제안드립니다.
              summary: Memory resources of the node {{ $labels.instance }} are running low.
              discriminative: $labels.taco_cluster, $labels.instance
            expr: (node_memory_MemAvailable_bytes/node_memory_MemTotal_bytes) <  0.2
            for: 3m
            labels:
              severity: warning
          - alert: node-disk-full
            annotations:
              message: 지난 6시간동안의 추세로 봤을 때, 클러스터({{ $labels.taco_cluster }})의 노드({{ $labels.instance }})의 root 볼륨은 24시간 안에 Disk full이 예상됨
              description: 현재 Disk 사용 추세기준 24시간 내에 Disk 용량이 꽉 찰 것으로 예상됩니다.
              Checkpoint: Disk 용량 최적화(삭제 및 Backup)을 수행하시길 권고합니다. 삭제할 내역이 없으면 증설 계획을 수립해 주십시요.
              summary: Memory resources of the node {{ $labels.instance }} are running low.
              discriminative: $labels.taco_cluster, $labels.instance
            expr: predict_linear(node_filesystem_free_bytes{mountpoint="/"}[6h], 24*3600) < 0
            for: 30m
            labels:
              severity: critical
          - alert: pvc-full
            annotations:
              message: 지난 6시간동안의 추세로 봤을 때, 클러스터({{ $labels.taco_cluster }})의 파드({{ $labels.persistentvolumeclaim }})가 24시간 안에 Disk full이 예상됨
              description: 현재 Disk 사용 추세기준 24시간 내에 Disk 용량이 꽉 찰것으로 예상됩니다. ({{ $labels.taco_cluster }} 클러스터, {{ $labels.persistentvolumeclaim }} PVC)
              Checkpoint: Disk 용량 최적화(삭제 및 Backup)을 수행하시길 권고합니다. 삭제할 내역이 없으면 증설 계획을 수립해 주십시요.
              summary: Disk resources of the volume(pvc) {{ $labels.persistentvolumeclaim }} are running low.
              discriminative: $labels.taco_cluster, $labels.persistentvolumeclaim
            expr: predict_linear(kubelet_volume_stats_available_bytes[6h], 24*3600) < 0 # kubelet_volume_stats_capacity_bytes
            for: 30m
            labels:
              severity: critical
          - alert: pod-restart-frequently
            annotations:
              message: 클러스터({{ $labels.taco_cluster }})의 파드({{ $labels.pod }})가 30분 동안 5회 이상 재기동 ({{ $value }}회)
              description: 특정 Pod가 빈번하게 재기동 되고 있습니다. 점검이 필요합니다. ({{ $labels.taco_cluster }} 클러스터, {{ $labels.pod }} 파드)
              Checkpoint: pod spec. 에 대한 점검이 필요합니다. pod의 log 및 status를 확인해 주세요.
              discriminative: $labels.taco_cluster, $labels.pod, $labels.namespace
            expr: increase(kube_pod_container_status_restarts_total{namespace!="kube-system"}[60m:]) > 2 # 몇회로 할 것인지?
            for: 30m
            labels:
              severity: critical

- name: thanos-config
  override:
    objectStorage:
      secretName: $(thanosObjstoreSecret)
      rawConfig:
        bucket: tks-thanos
        endpoint: $(s3Service)
        access_key: $(defaultUser)
        secret_key: $(defaultPassword)
        insecure: true
    sidecarsService.name: thanos-sidecars
    sidecarsService.endpoints:
      - 192.168.97.102 # should not be in the loopback range (127.0.0.0/8)

- name: prepare-etcd-secret
  override:
    nodeSelector:
      "node-role.kubernetes.io/control-plane": ""
    tolerations:
    - key: "node-role.kubernetes.io/control-plane"
      effect: "NoSchedule"
      operator: "Exists"
    - key: "node-role.kubernetes.io/master"
      effect: "NoSchedule"
      operator: "Exists"
    deployer: "tacoplay"

- name: loki
  override:
    global.dnsService: coredns
    global.clusterDomain: $(clusterName)
    gateway.service.type: ClusterIP
    gateway.service.annotations: $(awsNlbAnnotation)
    ingester.persistence.storageClass: $(storageClassName)
    distributor.persistence.storageClass: $(storageClassName)
    queryFrontend.persistence.storageClass: $(storageClassName)
    ruler.persistence.storageClass: $(storageClassName)
    indexGateway.persistence.storageClass: $(storageClassName)
    # select target node's label
    ingester.nodeSelector: $(nodeSelector)
    distributor.nodeSelector: $(nodeSelector)
    querier.nodeSelector: $(nodeSelector)
    queryFrontend.nodeSelector: $(nodeSelector)
    queryScheduler.nodeSelector: $(nodeSelector)
    tableManager.nodeSelector: $(nodeSelector)
    gateway.nodeSelector: $(nodeSelector)
    compactor.nodeSelector: $(nodeSelector)
    ruler.nodeSelector: $(nodeSelector)
    indexGateway.nodeSelector: $(nodeSelector)
    memcachedChunks.nodeSelector: $(nodeSelector)
    memcachedFrontend.nodeSelector: $(nodeSelector)
    memcachedIndexQueries.nodeSelector: $(nodeSelector)
    memcachedIndexWrites.nodeSelector: $(nodeSelector)
    loki:
      storageConfig:
        aws:
          s3: http://$(defaultUser):$(defaultPassword)@$(s3Service)/minio

- name: lma-bucket
  override:
    s3.enabled: true
    s3.buckets:
    - name: $(clusterName)-tks-thanos
    - name: $(clusterName)-tks-loki
    tks.iamRoles: $(tksIamRoles)

- name: eck-resource
  override:
    kibana.nodeSelector: $(nodeSelector)

    elasticsearch.nodeSets.master.nodeSelector: $(nodeSelector)
    elasticsearch.nodeSets.master.count: 3
    elasticsearch.nodeSets.master.javaOpts: "-Xms1g -Xmx1g"
    elasticsearch.nodeSets.master.limitCpu: 1
    elasticsearch.nodeSets.master.limitMem: 2Gi
    elasticsearch.nodeSets.master.pvc.storageClassName: $(storageClassName)
    elasticsearch.nodeSets.master.pvc.size: 1Gi

    elasticsearch.nodeSets.hotdata.nodeSelector: $(nodeSelector)
    elasticsearch.nodeSets.hotdata.count: 3
    elasticsearch.nodeSets.hotdata.javaOpts: "-Xms1g -Xmx1g"
    elasticsearch.nodeSets.hotdata.limitCpu: 1
    elasticsearch.nodeSets.hotdata.limitMem: 2Gi
    elasticsearch.nodeSets.hotdata.pvc.storageClassName: $(storageClassName)
    elasticsearch.nodeSets.hotdata.pvc.size: 10Gi

    elasticsearch.nodeSets.client.enabled: false
    elasticsearch.nodeSets.client.nodeSelector: $(nodeSelector)
    elasticsearch.nodeSets.client.javaOpts: "-Xms10g -Xmx10g"
    elasticsearch.nodeSets.client.limitCpu: 1
    elasticsearch.nodeSets.client.limitMem: 20Gi
    elasticsearch.nodeSets.client.pvc.storageClassName: $(storageClassName)
