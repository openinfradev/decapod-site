apiVersion: openinfradev.github.com/v1
kind: HelmValuesTransformer
metadata:
  name: site

global:
  clusterName: cluster.local
  storageClassName: taco-storage
  repository: https://openinfradev.github.io/hanu-helm-repo/
  externalIP: 172.16.53.26
  ceph_keyring: AQBAHAdfHQ5RLhAAzynxAYUokA55ku4ZwqVJjQ==
  cinder_keyring: AQAin8tU0CFgEhAATb7sYgtWsh+S5HEbg6MrGg==
  rbd_secret_uuid: 582393ff-9a5c-4a2e-ae0d-86ec18c36afc
  hypervisorInterface: eth0
  libvirtInterface: eth0
  adminPassword: Pa$$w0rd!

charts:
- name: ceph-provisioners
  source:
    repository: $(repository)
  override:
    deployment.ceph: false
    deployment.client_secrets: true
    deployment.rbd_provisioner: true
    deployment.cephfs_provisioner: false
    storageclass.rbd.provision_storage_class: true
    storageclass.cephfs.provision_storage_class: false
    conf.ceph.global.mon_host: TACO_MON_HOST
    manifests.configmap_bin: false
    manifests.configmap_bin_common: false
    manifests.configmap_etc: true
    manifests.deployment_rbd_provisioner: false
    manifests.deployment_cephfs_provisioner: false
    manifests.job_bootstrap: false
    manifests.job_cephfs_client_key: false
    manifests.job_image_repo_sync: false
    manifests.job_namespace_client_key_cleaner: false
    manifests.job_namespace_client_key: false
    manifests.storageclass_cephfs: false
    manifests.storageclass_rbd: false
    manifests.helm_tests: false
    endpoints.cluster_domain_suffix: $(clusterName)


- name: cinder
  source:
    repository: $(repository)
  override:
    pod.security_context.cinder_api.pod.runAsUser: 42407
    pod.security_context.cinder_backup.pod.runAsUser: 42407
    pod.security_context.cinder_scheduler.pod.runAsUser: 42407
    pod.security_context.cinder_volume.pod.runAsUser: 42407
    pod.replicas.api: 3
    pod.replicas.backup: 1
    pod.replicas.scheduler: 3
    pod.replicas.volume: 1
    #    conf.logging.loggers.keys: root
    #conf.logging.loggers.keys: cinder
    #conf.logging.loggers.keys: oslo_service
    conf.logging.logger_oslo_service.level: DEBUG
    conf.logging.logger_oslo_service.handlers: stderr
    conf.logging.logger_oslo_service.qualname: oslo_service
    conf.logging.logger_cinder.level: DEBUG
    conf.ceph.admin_keyring: $(ceph_keyring)
    conf.ceph.enabled: true
    conf.backends.rbd.rbd_pool: cinder-volumes
    conf.backends.rbd_secret_uuid: $(rbd_secret_uuid)
    conf.cinder.DEFAULT.enabled_backends: rbd
    conf.cinder.DEFAULT.default_volume_type: rbd
    conf.cinder.DEFAULT.backup_driver: cinder.backup.drivers.ceph.CephBackupDriver
    conf.cinder.DEFAULT.backup_ceph_user: cinder-backup
    conf.cinder.DEFAULT.backup_ceph_pool: backups
    conf.cinder.DEFAULT.debug: true
    endpoints.identity.auth.admin.username: admin
    endpoints.identity.auth.admin.password: $(adminPassword)
    endpoints.volume.path.default: /v2/%(tenant_id)s
    endpoints.cluster_domain_suffix: $(clusterName)

- name: glance
  source:
    repository: $(repository)
  override:
    pod.security_context.glance.pod.runAsUser: 42415
    pod.replicas.api: 1
    pod.replicas.registry: 1
    storage: $(storageClassName)
    conf.ceph.enabled: true
    conf.ceph.admin_keyring: $(ceph_keyring)
    conf.glance.glance_store.rbd_store_replication: 3
    conf.glance.glance_store.rbd_store_user: glance
    conf.glance.glance_store.rbd_store_pool: images
    conf.glance.DEFAULT.hw_scsi_model: virtio-scsi
    conf.glance.DEFAULT.hw_disk_bus: scsi
    conf.glance.DEFAULT.hw_qemu_guest_agent: yes
    conf.glance.DEFAULT.os_require_quiesce: yes
    conf.glance.DEFAULT.show_image_direct_url: true
    conf.glance.DEFAULT.show_multiple_locations: true
    bootstrap.enabled: true
    bootstrap.structured.images.cirros.name: Cirros 0.5.1 64-bit
    bootstrap.structured.images.cirros.id: 201084fc-c276-4744-8504-cb974dbb3610
    bootstrap.structured.images.cirros.source_url: https://download.cirros-cloud.net/0.5.1/
    bootstrap.structured.images.cirros.image_file: cirros-0.5.1-x86_64-disk.img
    bootstrap.structured.images.cirros.private: false
    endpoints.identity.auth.admin.username: admin
    endpoints.identity.auth.admin.password: $(adminPassword)
    endpoints.cluster_domain_suffix: $(clusterName)

- name: heat
  source:
    repository: $(repository)
  override:
    pod.security_context.heat.pod.runAsUser: 42418
    pod.replicas.api: 3
    pod.replicas.cfn: 3
    pod.replicas.cloudwatch: 3
    pod.replicas.engine: 3
    endpoints.identity.auth.admin.username: admin
    endpoints.identity.auth.admin.password: $(adminPassword)
    endpoints.cluster_domain_suffix: $(clusterName)

- name: horizon
  source:
    repository: $(repository)
  override:
    endpoints.cluster_domain_suffix: $(clusterName)
    pod.security_context.horizon.pod.runAsUser: 42420
    pod.replicas.server: 3
    network.node_port.enabled: true
    network.node_port.port: 31000
    conf.software.apache2.site_dir: /etc/apache2/sites-enabled
    conf.horizon.apache: |
        Listen 0.0.0.0:{{ tuple "dashboard" "internal" "web" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}
        LogFormat "%h %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" combined
        LogFormat "%{X-Forwarded-For}i %l %u %t \"%r\" %>s %b \"%{Referer}i\" \"%{User-Agent}i\"" proxy
        SetEnvIf X-Forwarded-For "^.*\..*\..*\..*" forwarded
        CustomLog /dev/stdout combined env=!forwarded
        CustomLog /dev/stdout proxy env=forwarded
        <VirtualHost *:{{ tuple "dashboard" "internal" "web" . | include "helm-toolkit.endpoints.endpoint_port_lookup" }}>
            WSGIScriptReloading On
            WSGIDaemonProcess horizon-http processes=5 threads=1 user=horizon group=horizon display-name=%{GROUP} python-path=/var/lib/kolla/venv/lib/python2.7/site-packages
            WSGIProcessGroup horizon-http
            WSGIScriptAlias / /var/www/cgi-bin/horizon/django.wsgi
            WSGIPassAuthorization On
            RewriteEngine on
            RewriteCond %{REQUEST_METHOD} !^(POST|PUT|GET|DELETE)
            RewriteRule .* - [F]
            <Location "/">
                Require all granted
            </Location>
            Alias /static /var/www/html/horizon
            <Location "/static">
                SetHandler None
            </Location>
            <IfVersion >= 2.4>
              ErrorLogFormat "%{cu}t %M"
            </IfVersion>
            ErrorLog /dev/stdout
            TransferLog /dev/stdout
            SetEnvIf X-Forwarded-For "^.*\..*\..*\..*" forwarded
            CustomLog /dev/stdout combined env=!forwarded
            CustomLog /dev/stdout proxy env=forwarded
        </Virtualhost>
    conf.horizon.local_settings.config.openstack_neutron_network.enable_router: "True"
    conf.horizon.local_settings.config.openstack_neutron_network.enable_quotas: "True"
    conf.horizon.local_settings.config.openstack_neutron_network.enable_ipv6: "False"
    conf.horizon.local_settings.config.openstack_neutron_network.enable_distributed_router: "False"
    conf.horizon.local_settings.config.openstack_neutron_network.enable_ha_router: "True"
    conf.horizon.local_settings.config.openstack_neutron_network.enable_lb: "True"
    conf.horizon.local_settings.config.openstack_neutron_network.enable_firewall: "False"
    conf.horizon.local_settings.config.openstack_neutron_network.enable_vpn: "False"
    conf.horizon.local_settings.config.openstack_neutron_network.enable_fip_topology_check: "True"
      
- name: ingress
  source:
    repository: $(repository)
  override:
    network.host_namespace: true
    network.vip.manage: false
    network.vip.mode: keepalived
    network.vip.interface: eth0
    network.vip.addr: 10.10.10.122/32
    network.vip.keepalived_router_id: 49
    monitoring.prometheus.enabled: true
    monitoring.prometheus.ingress_exporter.scrape: true
    monitoring.prometheus.config.worker-processes: 8
    config.worker-processes: 8
    pod.security_context.server.container.ingress_vip.capabilities.add: NET_ADMIN
    pod.security_context.server.container.ingress_vip.readOnlyRootFilesystem: false
    pod.security_context.server.container.ingress_vip.runAsUser: 0
    pod.replicas.ingress: 3
    pod.replicas.error_page: 1
    conf.ingress.bind-address: 0.0.0.0
    endpoints.ingress.port.server.default: 28080
    endpoints.cluster_domain_suffix: $(clusterName)

- name: keystone
  source:
    repository: $(repository)
  override:
    conf.keystone.DEFAULT.debug: true
    pod.security_context.keystone.pod.runAsUser: 42425
    pod.replicas.api: 1
    endpoints.identity.auth.admin.username: admin
    endpoints.identity.auth.admin.password: $(adminPassword)
    endpoints.cluster_domain_suffix: $(clusterName)
    pod.probes.api.api.readiness.enabled: false
    pod.probes.api.api.liveness.enabled: false
      
- name: libvirt
  source:
    repository: $(repository)
  override:
    release_group: null
    network.backend:
    - openvswitch
    conf.ceph.enabled: true
    conf.ceph.admin_keyring: $(ceph_keyring)
    conf.ceph.cinder.keyring: $(cinder_keyring)
    conf.ceph.cinder.secret_uuid: $(rbd_secret_uuid)
    conf.libvirt.listen_addr: 0.0.0.0
    conf.libvirt.log_level: 3
    manifests.configmap_bin: true
    manifests.configmap_etc: true
    manifests.daemonset_libvirt: true
    endpoints.cluster_domain_suffix: $(clusterName)

- name: openvswitch
  source:
    repository: $(repository)
  override:
    release_group: null
    pod.security_context.openvswitch_db_server.pod.runAsUser: 0
    pod.security_context.openvswitch_db_server.container.server.runAsUser: 0
    pod.user.nova.uid: 42436
    endpoints.cluster_domain_suffix: $(clusterName)

- name: mariadb
  source:
    repository: $(repository)
  override:
    pod.replicas.server: 1
    volume.size: 40Gi
    volume.enabled: true
    volume.class_name: rbd
    volume.backup.class_name: $(storageClassName)
    volume.backup.size: 10Gi
    volume.backup.enabled: false
    volume.class_name: $(storageClassName)
    endpoints.cluster_domain_suffix: $(clusterName)

- name: memcached
  source:
    repository: $(repository)
  override:
    pod.replicas.server: 3
    monitoring.prometheus.enabled: true
    monitoring.prometheus.memcached_exporter.scrape: true
    endpoints.cluster_domain_suffix: $(clusterName)
- name: neutron
  source:
    repository: $(repository)
  override:
    pod.replicas.server: 3
    pod.user.neutron.uid: 42435
    pod.security_context.neutron.pod.runAsUser: 42435
    pod.security_context.neutron_dhcp_agent.pod.runAsUser: 42435
    pod.security_context.neutron_dhcp_agent.container.neutron_dhcp_agent.readOnlyRootFilesystem: false
    pod.security_context.neutron_l2gw_agent.pod.runAsUser: 42435
    pod.security_context.neutron_bagpipe_bgp.pod.runAsUser: 42435
    pod.security_context.neutron_l3_agent.pod.runAsUser: 42435
    pod.security_context.neutron_l3_agent.container.neutron_l3_agent.readOnlyRootFilesystem: false
    pod.security_context.neutron_lb_agent.pod.runAsUser: 42435
    pod.security_context.neutron_lb_agent.container.neutron_lb_agent.readOnlyRootFilesystem: false
    pod.security_context.neutron_metadata_agent.pod.runAsUser: 42435
    pod.security_context.neutron_ovs_agent.pod.runAsUser: 42435
    pod.security_context.neutron_server.pod.runAsUser: 42435
    pod.security_context.neutron_server.container.neutron_server.readOnlyRootFilesystem: false
    pod.security_context.neutron_sriov_agent.pod.runAsUser: 42435
    pod.probes.dhcp_agent.dhcp_agent.readiness.enabled: false
    pod.probes.dhcp_agent.dhcp_agent.liveness.enabled: false
    pod.probes.l3_agent.l3_agent.readiness.enabled: false
    pod.probes.l3_agent.l3_agent.liveness.enabled: false
    pod.probes.metadata_agent.metadata_agent.readiness.enabled: false
    pod.probes.metadata_agent.metadata_agent.liveness.enabled: false
    pod.probes.ovs_agent.ovs_agent.liveness.enabled: false
    pod.probes.sriov_agent.sriov_agent.readiness.enabled: false
    dependencies.static.ovs_agent.pod: null
    network.backend:
    - openvswitch
    network.share_namespaces: true
    network.interface.tunnel: $(hypervisorInterface)
    conf.auto_bridge_add.br-ex:  $(libvirtInterface)
    conf.paste.composite:neutronapi_v2_0.keystone: cors http_proxy_to_wsgi request_id catch_errors authtoken keystonecontext extensions neutronapiapp_v2_0
    conf.paste.app:neutronversions.paste.app_factory: neutron.pecan_wsgi.app:versions_factory
    conf.neutron_sudoers: |
      # This sudoers file supports rootwrap-daemon for both Kolla and LOCI Images.
      Defaults !requiretty
      Defaults secure_path="/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin:/var/lib/openstack/bin:/var/lib/kolla/venv/bin"
      neutron ALL = (root) NOPASSWD: /var/lib/kolla/venv/bin/neutron-rootwrap /etc/neutron/rootwrap.conf *, /var/lib/openstack/bin/neutron-rootwrap /etc/neutron/rootwrap.conf *, /var/lib/kolla/venv/bin/neutron-rootwrap-daemon /etc/neutron/rootwrap.conf, /var/lib/openstack/bin/neutron-rootwrap-daemon /etc/neutron/rootwrap.conf, /var/lib/kolla/venv/bin/privsep-helper /etc/neutron/rootwrap.conf *
    conf.neutron.DEFAULT.core_plugin: ml2
    conf.neutron.DEFAULT.l3_ha: false
    conf.neutron.DEFAULT.global_physnet_mtu: 9000
    conf.neutron.DEFAULT.service_plugins: router
    conf.neutron.agent.root_helper: sudo /var/lib/kolla/venv/bin/neutron-rootwrap /etc/neutron/rootwrap.conf
    conf.neutron.placement.auth_version: v3
    conf.neutron.placement.auth_type: password
    conf.neutron.placement.auth_uri: http://keystone-api.openstack.svc.cluster.local:5000/v3
    conf.neutron.placement.auth_url: http://keystone-api.openstack.svc.cluster.local:5000/v3
    conf.neutron.placement.endpoint_type: internal
    conf.neutron.placement.project_domain_name: service
    conf.neutron.placement.project_name: service
    conf.neutron.placement.user_domain_name: service
    conf.neutron.placement.region_name: RegionOne
    conf.neutron.placement.password: t0pNw8pN4Qs93p1InK7XWDLz8CH7a08KZ6ClafHC
    conf.neutron.placement.username: nova
# using flat network instead of dvr. by jacob
#      conf.neutron.l3_agent.DEFAULT.agent_mode: dvr_snat
    conf.plugins.ml2_conf.ml2.mechanism_drivers: openvswitch,l2population
    conf.plugins.ml2_conf.ml2.type_drivers: vlan, flat, vxlan
    conf.plugins.ml2_conf.ml2.tenant_network_types: vxlan
# using vlan network instead of dvr. by jacob
#      conf.plugins.ml2_conf.ml2_type_vlan.network_vlan_ranges: "provider:110:110"
    conf.plugins.ml2_conf.ml2_type_flat.flat_networks: provider
    conf.plugins.openvswitch_agent.agent.tunnel_types: vxlan
    conf.plugins.openvswitch_agent.agent.l2_population: True
    conf.plugins.openvswitch_agent.agent.arp_responder: True
# using vlan network instead of dvr. by jacob
#      conf.plugins.openvswitch_agent.agent.enable_distributed_routing: True
    conf.plugins.openvswitch_agent.agent.enable_distributed_routing: False
    conf.plugins.openvswitch_agent.ovs.bridge_mappings: provider:br-ex
    conf.plugins.openvswitch_agent.securitygroup.firewall_driver: openvswitch
    endpoints.cluster_domain_suffix: $(clusterName)

- name: nova
  source:
    repository: $(repository)
  override:
    bootstrap.structured.flavors.enabled: true
    bootstrap.structured.flavors.options.m1_tiny.id: 0c84e220-a258-439f-a6ff-f8e9fd980025
    network.backend:
    - openvswitch
    network.novncproxy.name: nova-novncproxy
    network.novncproxy.node_port.enabled: true
    network.novncproxy.node_port.port: 30608
    console.novnc.compute.vncserver_proxyclient_interface: $(hypervisorInterface)
    console.novnc.vncproxy.vncserver_proxyclient_interface: $(hypervisorInterface)
    conf.hypervisor.host_interface: $(hypervisorInterface)
    conf.libvirt.live_migration_interface: $(hypervisorInterface)
    conf.ceph.enabled: true
    conf.ceph.admin_keyring: $(ceph_keyring)
    conf.ceph.cinder.user: cinder
    conf.ceph.cinder.keyring: $(cinder_keyring)
    conf.nova.DEFAULT.scheduler_default_filters: RetryFilter,AvailabilityZoneFilter,RamFilter,ComputeFilter,ComputeCapabilitiesFilter,ImagePropertiesFilter,ServerGroupAntiAffinityFilter,ServerGroupAffinityFilter
    conf.nova.DEFAULT.debug: true
    conf.nova.DEFAULT.dhcp_domain: ""
    conf.nova.DEFAULT.config_drive_cdrom: true
    conf.nova.DEFAULT.config_drive_format: iso9660
    conf.nova.DEFAULT.force_config_drive: true
    conf.nova.DEFAULT.ram_allocation_ratio: 0.8
    conf.nova.DEFAULT.disk_allocation_ratio: 9999.0
    conf.nova.DEFAULT.cpu_allocation_ratio: 8.0
    conf.nova.DEFAULT.osapi_compute_workers: 8
    conf.nova.vnc.novncproxy_base_url: http://$(externalIP):30608/vnc_auto.html
    conf.nova.libvirt.images_type: rbd
    conf.nova.libvirt.rbd_user: cinder
    conf.nova.libvirt.rbd_secret_uuid: $(rbd_secret_uuid)
    conf.nova.libvirt.virt_type: kvm
    conf.nova.scheduler.discover_hosts_in_cells_interval: 60
    conf.nova.placement_database.max_retries: -1
    conf.nova.placement_database.sync_on_startup: true
    conf.nova.placement.region_name: RegionOne
    conf.rootwrap_filters: null
    endpoints.identity.auth.admin.username: admin
    endpoints.identity.auth.admin.password: $(adminPassword)
    pod.mandatory_access_control.type: null
    pod.user.nova.uid: 42436
    pod.security_context.nova.pod.runAsUser: 42436
    pod.replicas.api_metadata: 3
    pod.replicas.osapi: 3
    pod.replicas.conductor: 3
    pod.replicas.consoleauth: 3
    pod.replicas.scheduler: 3
    pod.replicas.novncproxy: 3
    endpoints.cluster_domain_suffix: $(clusterName)

- name: rabbitmq
  source:
    repository: $(repository)
  override:
    pod.replicas.server: 3
#      volume.class_name: rbd
    volume.use_local_path.enabled: true
    volume.use_local_path.host_path: /ssd-data/rabbitmq-data
    volume.chown_on_start: true
    volume.enabled: false
    volume.size: 512Mi
    monitoring.prometheus.enabled: true
    monitoring.prometheus.rabbitmq_exporter.scrape: true
    endpoints.cluster_domain_suffix: $(clusterName)

#- name: prometheus-kube-state-metrics
#  source:
#    repository: $(repository)

#- name: grafana
#  source:
#    repository: $(repository)

#- name: prometheus
#  source:
#    repository: $(repository)
#  override:
#    endpoints.monitoring.auth.admin.username: admin
#    endpoints.monitoring.auth.admin.password: $(adminPassword)
#    storage.storage_class: $(storageClassName)
#    storage.requests.storage: 22Gi

#- name: prometheus-openstack-exporter
#  source:
#    repository: $(repository)
#  override:
#    endpoints.identity.name: keystone
#    endpoints.identity.auth.admin.region_name: RegionOne
#    endpoints.identity.auth.admin.username: admin
#    endpoints.identity.auth.admin.password: $(adminPassword)
#    dummy: dummy
#    pod.mandatory_access_control.type: null
